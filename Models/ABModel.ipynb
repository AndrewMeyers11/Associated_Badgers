{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fa8f0e",
   "metadata": {},
   "source": [
    "# Customer churn analysis\n",
    "\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "\n",
    "In the following notebooks, we will go through the implementation of each one of the steps in the Machine Learning Pipeline. \n",
    "\n",
    "We will discuss:\n",
    "\n",
    "1. Data Preparation and Analysis\n",
    "2. **Feature Engineering**\n",
    "3. **Feature Selection**\n",
    "4. **Model Training**\n",
    "5. **Obtaining Predictions / Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7750a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purkol\\anaconda3\\envs\\getting_started_snowpark_python\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.types import *\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from snowflake.snowpark.functions import udf\n",
    "%matplotlib inline\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to save the trained scaler class\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb1d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 11, 0)\n",
      "[Row(CURRENT_WAREHOUSE()='WH_UWM', CURRENT_DATABASE()='UWM_HOUSEHOLDING', CURRENT_SCHEMA()='TEAM1_XURAN')]\n"
     ]
    }
   ],
   "source": [
    "#Snowflake connection info\n",
    "from config import snowflake_conn_prop\n",
    "from snowflake.snowpark import version\n",
    "print(version.VERSION)\n",
    "\n",
    "session = Session.builder.configs(snowflake_conn_prop).create()\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5be4ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 547 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "raw = session.table('TABLE_FOR_TRAINING')\n",
    "data = raw.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81258f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s: str):\n",
    "        if \"None\" in s:\n",
    "            s = s.replace(\"None\", \" \")\n",
    "        elif re.search(r'Ave[.]*$', s):\n",
    "            s = s.replace(\"Ave\", \"Avenue\")\n",
    "        elif re.search(r'Av[.]*$', s):\n",
    "            s = s.replace(\"Av\", \"Avenue\")\n",
    "        elif re.search(r'St[.]*$', s):\n",
    "            s = s.replace(\"St\", \"Street\")\n",
    "        elif re.search(r'Rd[.]*$', s):\n",
    "            s = s.replace(\"Rd\", \"Road\")\n",
    "        elif re.search(r'Dr[.]*$', s):\n",
    "            s = s.replace('Dr', \"Drive\")\n",
    "        return s\n",
    "data[\"P1_ADDRESS_LINE_1\"]= data.apply(lambda row : encode(str(row[\"P1_ADDRESS_LINE_1\"])), axis = 1).map(str)\n",
    "data[\"P1_ADDRESS_LINE_3\"]= data.apply(lambda row : encode(str(row[\"P1_ADDRESS_LINE_3\"])), axis = 1).map(str)\n",
    "data[\"P2_ADDRESS_LINE_1\"]= data.apply(lambda row : encode(str(row[\"P2_ADDRESS_LINE_1\"])), axis = 1).map(str)\n",
    "data[\"P2_ADDRESS_LINE_3\"]= data.apply(lambda row : encode(str(row[\"P2_ADDRESS_LINE_3\"])), axis = 1).map(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f4f5d",
   "metadata": {},
   "source": [
    "# Separate dataset into train and test\n",
    "\n",
    "It is important to separate our data intro training and testing set. \n",
    "\n",
    "When we engineer features, some techniques learn parameters from data. It is important to learn these parameters only from the train set. This is to avoid over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334e90ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1601, 10), (401, 10), (1601,), (401,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's separate into train and test set\n",
    "# Remember to set the seed (random_state for this sklearn function)\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=['P1_CONTACT_ID','P2_CONTACT_ID',\n",
    "                       'P1_HOUSEHOLD_ID','P2_HOUSEHOLD_ID','IFFAMILY',\n",
    "                       'P1_FIRSTNAME', 'P1_MIDDLENAME','P2_FIRSTNAME', 'P2_MIDDLENAME',\n",
    "                       'P1_ADDRESS_LINE_2', 'P2_ADDRESS_LINE_2','P1_LASTNAME',  'P2_LASTNAME'], axis=13), # predictive variables\n",
    "    data['IFFAMILY'], # target\n",
    "    test_size=0.2, # portion of dataset to allocate to test set\n",
    "    random_state=2, # we are setting the seed here\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape,y_train.shape, y_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f9d84",
   "metadata": {},
   "source": [
    "## let's identify the different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecff006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P1_ADDRESS_LINE_1', 'P1_ADDRESS_LINE_3', 'P2_ADDRESS_LINE_1', 'P2_ADDRESS_LINE_3', 'P1_CITY', 'P1_STATE', 'P1_ZIP', 'P2_CITY', 'P2_STATE', 'P2_ZIP']\n"
     ]
    }
   ],
   "source": [
    "# Since we already cleaned up the data using snowpark, identifying variable is super easy\n",
    "\n",
    "# we will capture those of type *object*\n",
    "\n",
    "cat_vars = [ 'P1_ADDRESS_LINE_1', 'P1_ADDRESS_LINE_3','P2_ADDRESS_LINE_1',  'P2_ADDRESS_LINE_3', \n",
    "           'P1_CITY', 'P1_STATE','P1_ZIP','P2_CITY', 'P2_STATE','P2_ZIP']\n",
    "\n",
    "# we will capture those of type numerical from previous notebook\n",
    "num_vars = [ ]\n",
    "\n",
    "features = cat_vars + num_vars\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024540b",
   "metadata": {},
   "source": [
    "## Numerical variable transformation\n",
    "\n",
    "In the previous notebook, we observed that the numerical variables are not normally distributed.\n",
    "\n",
    "We will transform with the MinMaxScaler in order to get a more Gaussian-like distribution. Use ordinal encoding for the categorical variables and check for nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b0c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check absence of na in the train set\n",
    "[var for var in X_train.columns if X_train[var].isnull().sum() > 0]\n",
    "\n",
    "# check absence of na in the test set\n",
    "[var for var in X_test.columns if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f72712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pipeline\n",
    "\n",
    "#transformations\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Model Accuracy\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Model Pipeline\n",
    "ord_pipe = make_pipeline(\n",
    "    FunctionTransformer(lambda x: x.astype(str)) ,\n",
    "    OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    )\n",
    "\n",
    "num_pipe = make_pipeline(\n",
    "    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0),\n",
    "    MinMaxScaler()\n",
    "    )\n",
    "\n",
    "clf = make_pipeline(RandomForestClassifier(random_state=0, n_jobs=-1))\n",
    "\n",
    "model = make_pipeline(ord_pipe, num_pipe, clf)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd192895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "with open('model.pkl' ,'wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b432a7",
   "metadata": {},
   "source": [
    "## Check Accuracy of our model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "predictions = [str(round(value))for value in y_pred]\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"Model testing completed.\\n   - Model Balanced Accuracy: %.2f%%\" % ((TP+TN) /(TP+FP+TN+FN) * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d32c9",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a285b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP+TN) /(TP+FP+TN+FN)\n",
    "\n",
    "print('Accuracy of the classification = {:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1f017",
   "metadata": {},
   "source": [
    "## Check for important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_importance = permutation_importance(model, X_test, y_test)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(np.array(X_test.columns)[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42769e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getting_started_snowpark_python",
   "language": "python",
   "name": "getting_started_snowpark_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
